{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f794a91",
   "metadata": {},
   "source": [
    "## Analyzing Research Articles using Topic Modelling Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing neccessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3047e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and read datasets\n",
    "\n",
    "papers = pd.read_excel('C:/Users/Saleeh/Documents/Salford University/Lectures/AI - Natural Language Processing/Assessment/Dataset/Scopus Extract/Scopus dataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1310a6f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View first 5 rows of data \n",
    "\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd87a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92445775",
   "metadata": {},
   "source": [
    "Datasets has different number of unique values for title and abstract and total number of records indicating that some documents may have similar titles or abstracts across different subject classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470d095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of the columns in the dataset\n",
    "# Totals rows of that is 303\n",
    "\n",
    "papers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns that does not relate to the task and select chunck of the data (sample >> 100)\n",
    "# Add paper_text column\n",
    "\n",
    "papers = papers.drop(columns=['Authors','Source title','Link','Subject'], axis=1).sample(100)\n",
    "\n",
    "#print out the first five rows of papers\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54d729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add paper_text column\n",
    "papers['paper_text'] = papers['Title'] + \" \" + papers['Abstract']\n",
    "\n",
    "#print out the first five rows of papers\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643654c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove missing values\n",
    "\n",
    "papers = papers.dropna(axis='rows')\n",
    "\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b3de2",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1685cdfb",
   "metadata": {},
   "source": [
    "The Text preprocessing step will be carried out in the following step\n",
    "\n",
    "1. Data Cleaning\n",
    "2. Tokenization\n",
    "3. Stopword Removal\n",
    "4. Normalization\n",
    "5. Text Encoding\n",
    "6. Vectorization\n",
    "7. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50339b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import neccessary libraries for text preprocessing\n",
    "\n",
    "import re\n",
    "from string import punctuation\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d860464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function for preprocessing\n",
    "\n",
    "def clean_text(text):\n",
    "    # make text lowercase    \n",
    "    text = str(text).lower()\n",
    "    # expand contractions\n",
    "    text = \" \".join([contractions.fix(expanded_word) for expanded_word in text.split()])\n",
    "    # remove text in square brackets\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    # remove links\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    # remove punctuation\n",
    "    text = re.sub('[%s]' % re.escape(punctuation), '', text)\n",
    "    # remove new lines\n",
    "    text = re.sub('\\n', '', text)\n",
    "    # remove words containing numbers\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    # remove apostrophes\n",
    "    text = re.sub(\"''\", '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e251a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply function on each text in the dataset\n",
    "\n",
    "papers['paper_text_preprocessed'] = papers['paper_text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a7faf7",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bde9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add comment\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "papers['paper_words'] = papers['paper_text_preprocessed'].apply(lambda x:word_tokenize(str(x)))\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93dbf04",
   "metadata": {},
   "source": [
    "### Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699833ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0244e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stopword removal function\n",
    "\n",
    "def remove_stopword(word_list):\n",
    "    return [word for word in word_list if word not in stopwords.words('english')]\n",
    "\n",
    "papers['paper_words_excl_sw'] = papers['paper_words'].apply(lambda x: remove_stopword(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae372e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa813d7a",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba21426",
   "metadata": {},
   "source": [
    "After removing stopwords, I applied lemmatization technique to further preprocces the text data bu converting each word to its base form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23dca59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "papers['paper_words_excl_sw'] = papers['paper_words_excl_sw'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "papers.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5979e2",
   "metadata": {},
   "source": [
    "## Visualize the data using wordcloud package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data using wordcloud package\n",
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Join the different processed titles together\n",
    "long_string = ' '.join((str(l) for l in papers['paper_words_excl_sw']))\n",
    "                       \n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "#Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "#Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034e9b0d",
   "metadata": {},
   "source": [
    "## LDA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c1d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "        \n",
    "data = papers['paper_words_excl_sw'].tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "\n",
    "print(data_words[:1][0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1][0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71459e10",
   "metadata": {},
   "source": [
    "## LDA model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1629f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81de8bb",
   "metadata": {},
   "source": [
    "## Analyzing LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd83e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import os\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('/content_'+str(num_topics))\n",
    "\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'content_'+ str(num_topics) +'.html')\n",
    "\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577b3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
